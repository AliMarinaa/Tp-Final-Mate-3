{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Fordward Propagation\n",
    "\n",
    "- Se consideran todas las columnas menos Death Event para las entradas de la red neuronal.\n",
    "\n",
    "- Se normalizan los datos con StandardScaler\n",
    "\n",
    "- Dos capas ocultas, la primera con 10 neuronas y la segunda con 5, Función de activación ReLU.\n",
    "\n",
    "- 5000 máximo de iteraciones para el entrenamiento\n",
    "\n",
    "- L = 0.01 tasa de aprendizaje\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.3065386922615477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('heart_failure_clinical_records.csv', delimiter=',', decimal='.')\n",
    "df.columns = ['Age','Anaemia','CPK','Diabetes','Ejection Fraction','High Blood Pressure','Platelets','Serum Creatinine','Serum Sodium','Sex','Smoking','Time','Death Event']\n",
    "\n",
    "X = df.drop(columns=['Death Event']).values\n",
    "Y = df['Death Event'].values\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=42)\n",
    "\n",
    "# Número de registros de entrenamiento\n",
    "n = X_train.shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "# Inicializar pesos y sesgos para las capas\n",
    "# Capa 1 -> Capa 2\n",
    "w_hidden1 = np.random.rand(10, 12)\n",
    "b_hidden1 = np.random.rand(10, 1)\n",
    "\n",
    "# Capa 2 -> Capa 3 (nueva capa intermedia)\n",
    "w_hidden2 = np.random.rand(5, 10)\n",
    "b_hidden2 = np.random.rand(5, 1)\n",
    "\n",
    "# Capa 3 -> Salida\n",
    "w_output = np.random.rand(1, 5)\n",
    "b_output = np.random.rand(1, 1)\n",
    "\n",
    "# Funciones de activación\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "relu_derivative = lambda x: (x > 0).astype(float)\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "logistic_derivative = lambda x: logistic(x) * (1 - logistic(x))\n",
    "\n",
    "# Función de propagación hacia adelante\n",
    "def forward_prop(X):\n",
    "    Z1 = w_hidden1 @ X + b_hidden1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = w_hidden2 @ A1 + b_hidden2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = w_output @ A2 + b_output\n",
    "    A3 = logistic(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[5] \n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "#El valor de precisión (accuracy) significa que aproximadamente el 30,65%% de las predicciones \n",
    "# realizadas por la red neuronal en el conjunto de prueba (X_test) fueron correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.9412117576484703\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L = 0.01  # La tasa de aprendizaje\n",
    "\n",
    "# Devuelve pendientes para pesos y sesgos\n",
    "# usando la regla de la cadena\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "    dC_dA3 = 2 * (A3 - Y)\n",
    "    dA3_dZ3 = logistic_derivative(Z3)\n",
    "    dZ3_dW3 = A2\n",
    "    dZ3_dA2 = w_output\n",
    "    dC_dZ3 = dC_dA3 * dA3_dZ3\n",
    "\n",
    "    dC_dW3 = dC_dZ3 @ dZ3_dW3.T\n",
    "    dC_dB3 = np.sum(dC_dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dA2 = dZ3_dA2.T @ dC_dZ3\n",
    "    dA2_dZ2 = relu_derivative(Z2)\n",
    "    dZ2_dW2 = A1\n",
    "    dZ2_dA1 = w_hidden2\n",
    "    dC_dZ2 = dC_dA2 * dA2_dZ2\n",
    "\n",
    "    dC_dW2 = dC_dZ2 @ dZ2_dW2.T\n",
    "    dC_dB2 = np.sum(dC_dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dA1 = dZ2_dA1.T @ dC_dZ2\n",
    "    dA1_dZ1 = relu_derivative(Z1)\n",
    "    dZ1_dW1 = X\n",
    "    dC_dZ1 = dC_dA1 * dA1_dZ1\n",
    "\n",
    "    dC_dW1 = dC_dZ1 @ dZ1_dW1.T\n",
    "    dC_dB1 = np.sum(dC_dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n",
    "\n",
    "# Ejecutar descenso de gradiente\n",
    "for i in range(100000):\n",
    "    # seleccionar aleatoriamente un conjunto de datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx].reshape(1, 1)\n",
    "\n",
    "    # pasar datos seleccionados aleatoriamente a través de la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    # distribuir error a través de la retropropagación y devolver pendientes para pesos y sesgos\n",
    "    dW1, dB1, dW2, dB2, dW3, dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "\n",
    "    # Actualizar pesos y sesgos\n",
    "    w_hidden1 -= L * dW1\n",
    "    b_hidden1 -= L * dB1\n",
    "    w_hidden2 -= L * dW2\n",
    "    b_hidden2 -= L * dB2\n",
    "    w_output -= L * dW3\n",
    "    b_output -= L * dB3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[5]\n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "\n",
    "# El valor de precisión (accuracy) es 94,12% (mayor q el anterior) indica la proporción  de predicciones correctas realizadas \n",
    "# por la red neuronal en el conjunto de prueba (X_test) después de entrenarla mediante el descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Fordward Propagation\n",
    "\n",
    "- Se consideran solo las columnas que me interesan para las entradas de la red neuronal.\n",
    "Las variables que muestran una correlación moderada con la variable que me interesa Death Event son: Age / Ejection Fraction / Serum Creatinine / Serum.Sodium / Time. De esta forma evitamos la variables Platelets ya que este presentaba mayor cantidad de valores atipicos.\n",
    "\n",
    "- Se normalizan los datos con StandardScaler\n",
    "\n",
    "- Dos capas ocultas, la primera con 10 neuronas y la segunda con 5, Función de activación ReLU.\n",
    "\n",
    "- 5000 máximo de iteraciones para el entrenamiento\n",
    "\n",
    "- L = 0.01 tasa de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.3065386922615477\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = ['Age','Ejection Fraction', 'Serum Creatinine','Serum Sodium', 'Time']\n",
    "\n",
    "X = df[data].values\n",
    "Y = df['Death Event'].values\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=42)\n",
    "\n",
    "# Número de registros de entrenamiento\n",
    "n = X_train.shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "# Inicializar pesos y sesgos para las capas\n",
    "# Capa 1 -> Capa 2\n",
    "w_hidden1 = np.random.rand(10, 5)\n",
    "b_hidden1 = np.random.rand(10, 1)\n",
    "\n",
    "# Capa 2 -> Capa 3 (nueva capa intermedia)\n",
    "w_hidden2 = np.random.rand(5, 10)\n",
    "b_hidden2 = np.random.rand(5, 1)\n",
    "\n",
    "# Capa 3 -> Salida\n",
    "w_output = np.random.rand(1, 5)\n",
    "b_output = np.random.rand(1, 1)\n",
    "\n",
    "# Funciones de activación\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "relu_derivative = lambda x: (x > 0).astype(float)\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "logistic_derivative = lambda x: logistic(x) * (1 - logistic(x))\n",
    "\n",
    "# Función de propagación hacia adelante\n",
    "def forward_prop(X):\n",
    "    Z1 = w_hidden1 @ X + b_hidden1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = w_hidden2 @ A1 + b_hidden2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = w_output @ A2 + b_output\n",
    "    A3 = logistic(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[5] \n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "#El valor de precisión (accuracy) significa que aproximadamente el 30,65%% de las predicciones \n",
    "# realizadas por la red neuronal en el conjunto de prueba (X_test) fueron correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.9490101979604079\n"
     ]
    }
   ],
   "source": [
    "L = 0.01  # La tasa de aprendizaje\n",
    "\n",
    "# Devuelve pendientes para pesos y sesgos\n",
    "# usando la regla de la cadena\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "    dC_dA3 = 2 * (A3 - Y)\n",
    "    dA3_dZ3 = logistic_derivative(Z3)\n",
    "    dZ3_dW3 = A2\n",
    "    dZ3_dA2 = w_output\n",
    "    dC_dZ3 = dC_dA3 * dA3_dZ3\n",
    "\n",
    "    dC_dW3 = dC_dZ3 @ dZ3_dW3.T\n",
    "    dC_dB3 = np.sum(dC_dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dA2 = dZ3_dA2.T @ dC_dZ3\n",
    "    dA2_dZ2 = relu_derivative(Z2)\n",
    "    dZ2_dW2 = A1\n",
    "    dZ2_dA1 = w_hidden2\n",
    "    dC_dZ2 = dC_dA2 * dA2_dZ2\n",
    "\n",
    "    dC_dW2 = dC_dZ2 @ dZ2_dW2.T\n",
    "    dC_dB2 = np.sum(dC_dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dA1 = dZ2_dA1.T @ dC_dZ2\n",
    "    dA1_dZ1 = relu_derivative(Z1)\n",
    "    dZ1_dW1 = X\n",
    "    dC_dZ1 = dC_dA1 * dA1_dZ1\n",
    "\n",
    "    dC_dW1 = dC_dZ1 @ dZ1_dW1.T\n",
    "    dC_dB1 = np.sum(dC_dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n",
    "\n",
    "# Ejecutar descenso de gradiente\n",
    "for i in range(100000):\n",
    "    # seleccionar aleatoriamente un conjunto de datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx].reshape(1, 1)\n",
    "\n",
    "    # pasar datos seleccionados aleatoriamente a través de la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    # distribuir error a través de la retropropagación y devolver pendientes para pesos y sesgos\n",
    "    dW1, dB1, dW2, dB2, dW3, dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "\n",
    "    # Actualizar pesos y sesgos\n",
    "    w_hidden1 -= L * dW1\n",
    "    b_hidden1 -= L * dB1\n",
    "    w_hidden2 -= L * dW2\n",
    "    b_hidden2 -= L * dB2\n",
    "    w_output -= L * dW3\n",
    "    b_output -= L * dB3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[5]\n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "# El valor de precisión (accuracy) es 94,90% (mayor q el anterior) indica la proporción  de predicciones correctas realizadas \n",
    "# por la red neuronal en el conjunto de prueba (X_test) después de entrenarla mediante el descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Fordward Propagation\n",
    "\n",
    "- Se consideran solo las columnas que me interesan para las entradas de la red neuronal.\n",
    "Las variables que muestran una correlación moderada con la variable que me interesa Death Event son: Age / Ejection Fraction / Serum Creatinine / Serum.Sodium / Time. De esta forma evitamos la variables Platelets ya que este presentaba mayor cantidad de valores atipicos.\n",
    "\n",
    "- Normalizo las columnas que no estan entre 0 y 1, dividiendo por el maximo de cada columna.\n",
    "\n",
    "- Dos capas ocultas, la primera con 10 neuronas y la segunda con 5, Función de activación ReLU.\n",
    "\n",
    "- 5000 máximo de iteraciones para el entrenamiento\n",
    "\n",
    "- L = 0.01 tasa de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Ejection Fraction</th>\n",
       "      <th>Serum Creatinine</th>\n",
       "      <th>Serum Sodium</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.634618</td>\n",
       "      <td>0.471683</td>\n",
       "      <td>0.145650</td>\n",
       "      <td>0.924380</td>\n",
       "      <td>0.458522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.123129</td>\n",
       "      <td>0.143936</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.030164</td>\n",
       "      <td>0.271319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.763514</td>\n",
       "      <td>0.014035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.259649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.396491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.705263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age  Ejection Fraction  Serum Creatinine  Serum Sodium  \\\n",
       "count  5000.000000        5000.000000       5000.000000   5000.000000   \n",
       "mean      0.634618           0.471683          0.145650      0.924380   \n",
       "std       0.123129           0.143936          0.107420      0.030164   \n",
       "min       0.421053           0.175000          0.053191      0.763514   \n",
       "25%       0.526316           0.375000          0.095745      0.905405   \n",
       "50%       0.631579           0.475000          0.117021      0.925676   \n",
       "75%       0.715789           0.562500          0.148936      0.945946   \n",
       "max       1.000000           1.000000          1.000000      1.000000   \n",
       "\n",
       "              Time  \n",
       "count  5000.000000  \n",
       "mean      0.458522  \n",
       "std       0.271319  \n",
       "min       0.014035  \n",
       "25%       0.259649  \n",
       "50%       0.396491  \n",
       "75%       0.705263  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_norm= ['Age', 'CPK', 'Ejection Fraction', 'Platelets', 'Serum Creatinine', 'Serum Sodium','Time']\n",
    "\n",
    "#Normalizó dividiendo cada columna por el maximo valor de esa columna para que de como resultado valores entre 0 y 1\n",
    "df[column_norm] = df[column_norm] / df[column_norm].max()\n",
    "data = ['Age','Ejection Fraction', 'Serum Creatinine','Serum Sodium', 'Time'] \n",
    "df[data].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.3065386922615477\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[data].values\n",
    "Y = df['Death Event'].values\n",
    "\n",
    "# Normalizar los datos\n",
    "#scaler = StandardScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=42)\n",
    "\n",
    "# Número de registros de entrenamiento\n",
    "n = X_train.shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "# Inicializar pesos y sesgos para las capas\n",
    "# Capa 1 -> Capa 2\n",
    "w_hidden1 = np.random.rand(10, 5)\n",
    "b_hidden1 = np.random.rand(10, 1)\n",
    "\n",
    "# Capa 2 -> Capa 3 (nueva capa intermedia)\n",
    "w_hidden2 = np.random.rand(5, 10)\n",
    "b_hidden2 = np.random.rand(5, 1)\n",
    "\n",
    "# Capa 3 -> Salida\n",
    "w_output = np.random.rand(1, 5)\n",
    "b_output = np.random.rand(1, 1)\n",
    "\n",
    "# Funciones de activación\n",
    "#relu = lambda x: np.maximum(x, 0)\n",
    "#logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Funciones de activación\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "relu_derivative = lambda x: (x > 0).astype(float)\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "logistic_derivative = lambda x: logistic(x) * (1 - logistic(x))\n",
    "\n",
    "# Función de propagación hacia adelante\n",
    "def forward_prop(X):\n",
    "    Z1 = w_hidden1 @ X + b_hidden1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = w_hidden2 @ A1 + b_hidden2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = w_output @ A2 + b_output\n",
    "    A3 = logistic(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[5] \n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "#El valor de precisión (accuracy) significa que aproximadamente el 30,65%% de las predicciones \n",
    "# realizadas por la red neuronal en el conjunto de prueba (X_test) fueron correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.3065386922615477\n"
     ]
    }
   ],
   "source": [
    "L = 0.01  # La tasa de aprendizaje\n",
    "\n",
    "# Devuelve pendientes para pesos y sesgos\n",
    "# usando la regla de la cadena\n",
    "def backward_prop(Z1, A1, Z2, A2, Z3, A3, X, Y):\n",
    "    dC_dA3 = 2 * (A3 - Y)\n",
    "    dA3_dZ3 = logistic_derivative(Z3)\n",
    "    dZ3_dW3 = A2\n",
    "    dZ3_dA2 = w_output\n",
    "    dC_dZ3 = dC_dA3 * dA3_dZ3\n",
    "\n",
    "    dC_dW3 = dC_dZ3 @ dZ3_dW3.T\n",
    "    dC_dB3 = np.sum(dC_dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dA2 = dZ3_dA2.T @ dC_dZ3\n",
    "    dA2_dZ2 = relu_derivative(Z2)\n",
    "    dZ2_dW2 = A1\n",
    "    dZ2_dA1 = w_hidden2\n",
    "    dC_dZ2 = dC_dA2 * dA2_dZ2\n",
    "\n",
    "    dC_dW2 = dC_dZ2 @ dZ2_dW2.T\n",
    "    dC_dB2 = np.sum(dC_dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dC_dA1 = dZ2_dA1.T @ dC_dZ2\n",
    "    dA1_dZ1 = relu_derivative(Z1)\n",
    "    dZ1_dW1 = X\n",
    "    dC_dZ1 = dC_dA1 * dA1_dZ1\n",
    "\n",
    "    dC_dW1 = dC_dZ1 @ dZ1_dW1.T\n",
    "    dC_dB1 = np.sum(dC_dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dC_dW1, dC_dB1, dC_dW2, dC_dB2, dC_dW3, dC_dB3\n",
    "\n",
    "# Ejecutar descenso de gradiente\n",
    "for i in range(100000):\n",
    "    # seleccionar aleatoriamente un conjunto de datos de entrenamiento\n",
    "    idx = np.random.choice(n, 1, replace=False)\n",
    "    X_sample = X_train[idx].transpose()\n",
    "    Y_sample = Y_train[idx].reshape(1, 1)\n",
    "\n",
    "    # pasar datos seleccionados aleatoriamente a través de la red neuronal\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(X_sample)\n",
    "\n",
    "    # distribuir error a través de la retropropagación y devolver pendientes para pesos y sesgos\n",
    "    dW1, dB1, dW2, dB2, dW3, dB3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, X_sample, Y_sample)\n",
    "\n",
    "    # Actualizar pesos y sesgos\n",
    "    w_hidden1 -= L * dW1\n",
    "    b_hidden1 -= L * dB1\n",
    "    w_hidden2 -= L * dW2\n",
    "    b_hidden2 -= L * dB2\n",
    "    w_output -= L * dW3\n",
    "    b_output -= L * dB3\n",
    "\n",
    "# Cálculo de precisión\n",
    "test_predictions = forward_prop(X_test.transpose())[5]\n",
    "test_predictions = (test_predictions >= 0.5).astype(int)  # Convertir las predicciones en valores binarios\n",
    "accuracy = np.mean(test_predictions == Y_test.reshape(1, -1))  # Calcular la precisión comparando las predicciones con los valores reales\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "# El valor de precisión (accuracy) es 30,65% indica la proporción  de predicciones correctas realizadas \n",
    "# por la red neuronal en el conjunto de prueba (X_test) después de entrenarla mediante el descenso de gradiente.\n",
    "# ¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿¿Es igual que el anterior porque??????????????????????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
